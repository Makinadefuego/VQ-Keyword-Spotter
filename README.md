# üó£Ô∏è Clasificador de Palabras Clave por Cuantizaci√≥n Vectorial (VQ)

![Python](https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge&logo=python)
![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white)
![Librosa](https://img.shields.io/badge/Librosa-%23FF4800.svg?style=for-the-badge&logo=librosa&logoColor=white)
![CustomTkinter](https://img.shields.io/badge/CustomTkinter-%233A7EBF.svg?style=for-the-badge)

Este proyecto es un sistema de **reconocimiento de palabras clave aisladas** (Keyword Spotting) construido desde cero en Python. Utiliza t√©cnicas cl√°sicas de procesamiento de se√±ales y un clasificador basado en **Cuantizaci√≥n Vectorial (VQ)** para identificar un vocabulario predefinido de palabras a partir de grabaciones de audio.

El sistema est√° dise√±ado para ser **robusto al ruido** y a la variabilidad del hablante, gracias al uso de un "Super-Vector" de caracter√≠sticas avanzadas y t√©cnicas de aumento de datos. Incluye una interfaz gr√°fica moderna para el reconocimiento en tiempo real.

## ‚ú® Caracter√≠sticas Principales

-   **Clasificador VQ:** Utiliza K-Means para crear un "codebook" (huella ac√∫stica) √∫nico para cada palabra.
-   **Extracci√≥n de Caracter√≠sticas Avanzada:** Construye un "Super-Vector" por trama de audio, combinando:
    -   MFCCs (con Deltas y Delta-Deltas)
    -   GFCCs (alternativa robusta a los MFCCs)
    -   Energ√≠a, Pitch y Probabilidad de Voz
    -   Caracter√≠sticas Espectrales (Est√°ndar y Avanzadas como curtosis, pendiente, rolloff, etc.)
-   **Robustez al Ruido:**
    -   **Aumento de Datos:** Genera autom√°ticamente nuevas muestras de entrenamiento con ruido de fondo, cambios de tono y variaciones de velocidad.
    -   **Modelo de Basura:** Capacidad de entrenar un modelo `_garbage_` para rechazar activamente palabras desconocidas y ruidos.
-   **Interfaz Gr√°fica Moderna:** Una GUI intuitiva construida con `CustomTkinter` para el reconocimiento en tiempo real.
-   **Altamente Configurable:** Todos los par√°metros del sistema (caracter√≠sticas, modelo, rutas, etc.) se gestionan centralmente en el archivo `config.py`.

## üèõÔ∏è Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ VOICE/                # 1. Tus grabaciones de voz originales van aqu√≠.
‚îú‚îÄ‚îÄ background_noises/    # 2. Archivos de ruido para el aumento de datos.
‚îú‚îÄ‚îÄ dataset/              # 3. Creado autom√°ticamente (train/test sets).
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îú‚îÄ‚îÄ models/               # 4. El modelo entrenado (.joblib) se guarda aqu√≠.
‚îÇ
‚îú‚îÄ‚îÄ gui_recognizer.py     # ‚úÖ La aplicaci√≥n principal con interfaz gr√°fica.
‚îú‚îÄ‚îÄ train.py              # Script para entrenar el modelo.
‚îú‚îÄ‚îÄ augment_dataset.py    # Script para generar datos aumentados.
‚îú‚îÄ‚îÄ prepare_dataset.py    # Script para dividir el dataset.
‚îú‚îÄ‚îÄ evaulate.py           # Script para medir la precisi√≥n del modelo.
‚îÇ
‚îú‚îÄ‚îÄ config.py             # Archivo de configuraci√≥n central.
‚îú‚îÄ‚îÄ vq_classifier.py      # L√≥gica del clasificador VQ.
‚îú‚îÄ‚îÄ feature_extractor.py  # L√≥gica de extracci√≥n de caracter√≠sticas.
‚îî‚îÄ‚îÄ audio_utils.py        # Funciones auxiliares para c√°lculos de audio.
```

---

## üöÄ Gu√≠a de Inicio R√°pido

### 1. Configuraci√≥n del Entorno

**a) Clona el repositorio (si aplica):**
```bash
git clone https://github.com/Makinadefuego/VQ-Keyword-Spotter.git
cd tu_repositorio
```

**b) Instala todas las dependencias:**
Abre una terminal en la carpeta del proyecto y ejecuta:
```bash
pip install numpy librosa scipy scikit-learn joblib tqdm sounddevice soundfile audiomentations gammatone Pillow customtkinter
```

**c) Prepara las carpetas y recursos:**
-   Crea una carpeta llamada `VOICE`.
-   Crea una carpeta llamada `background_noises` y ll√©nala con algunos archivos de ruido de fondo en formato `.wav` (ej. ruido de oficina, calle, cafeter√≠a).
-   Descarga un √≠cono de micr√≥fono (`.png` con transparencia) y gu√°rdalo como `mic_icon.png` en la ra√≠z del proyecto.

### 2. Flujo de Trabajo

Sigue estos pasos en orden para entrenar y ejecutar tu propio reconocedor.

#### **Paso 1: Graba tus Palabras**

-   Graba cada palabra de tu vocabulario varias veces (se recomiendan m√°s de 10 por palabra).
-   Guarda los archivos en la carpeta `./VOICE`.
-   **Nombra los archivos** de forma que la palabra clave est√© al principio. Por ejemplo: `abrir_1.wav`, `abrir_user2.m4a`, `cerrar_intento3.wav`.

**Consejo Pro:** Para implementar el rechazo de palabras, graba sonidos que no pertenezcan a tu vocabulario (otras palabras, ruidos, toses) y n√≥mbralos como `_garbage_1.wav`, `_garbage_2.wav`, etc.

#### **Paso 2: Prepara el Dataset**

Este script divide tus grabaciones en un conjunto de entrenamiento (80%) y uno de prueba (20%).
```bash
python prepare_dataset.py
```
Se crear√° la carpeta `./dataset` con los sets de `train` y `test`.

#### **Paso 3: Aumenta los Datos**

Este paso es crucial para la robustez. Genera nuevas muestras de audio con variaciones.
```bash
python augment_dataset.py
```
Tu carpeta `dataset/train` ahora contendr√° muchas m√°s muestras, etiquetadas con `_aug_`.

#### **Paso 4: Entrena el Modelo**

Ahora, el sistema aprender√° las "huellas ac√∫sticas" de cada palabra a partir del dataset de entrenamiento enriquecido.
```bash
python train.py
```
El modelo entrenado se guardar√° como `vq_robust_model.joblib` en la carpeta `./models`.

#### **Paso 5: ¬°Ejecuta la Aplicaci√≥n!**

Lanza la interfaz gr√°fica para el reconocimiento en tiempo real.
```bash
python gui.py
```
Haz clic en el √≠cono del micr√≥fono, di una de tus palabras clave y observa el resultado. La consola te mostrar√° logs detallados de todo el proceso.

---

## üõ†Ô∏è Personalizaci√≥n y Uso Avanzado

### Evaluar el Rendimiento

Para medir qu√© tan bueno es tu modelo, ejec√∫talo contra el set de datos de prueba. Esto te dar√° la precisi√≥n y una matriz de confusi√≥n para ver d√≥nde falla.
```bash
python evaulate.py
```

### Ajustar el Sistema (`config.py`)

El archivo `config.py` es el centro de control. Puedes experimentar cambiando:
-   **`FEATURES`**: Activa o desactiva diferentes tipos de caracter√≠sticas (`use_mfcc`, `use_gfcc`) para ver c√≥mo impactan en la precisi√≥n.
-   **`MODEL['vq_clusters']`**: Aumenta este valor (ej. a 128) para un modelo m√°s detallado (requiere m√°s datos y tiempo), o red√∫celo (ej. a 32) para un modelo m√°s simple.
-   **`AUGMENTATION`**: Modifica la cantidad y la intensidad de las transformaciones de aumento de datos.
-   **`GUI['vad_aggressiveness']`**: En futuras versiones con VAD, este par√°metro ser√≠a clave.

**Importante:** Despu√©s de cualquier cambio en `config.py` que afecte a las caracter√≠sticas o al modelo, **debes borrar el modelo antiguo y volver a entrenar** (`python train.py`).

### Reconocer un √önico Archivo

Si solo quieres clasificar un archivo de audio espec√≠fico desde la terminal, puedes usar el script `recognize.py` (si lo mantienes en el proyecto).
```bash
python recognize.py /ruta/a/tu/audio.wav
```

## üìú Licencia

Este proyecto est√° bajo la Licencia MIT. Consulta el archivo `LICENSE` para m√°s detalles.

---

MDF, raya_cuadernos